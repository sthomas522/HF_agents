{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sthomas522/HF_agents/blob/main/experiments/GenAI_Final_Project_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d053c1d0",
      "metadata": {
        "id": "d053c1d0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sthomas522/HF_agents.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd HF_agents\n",
        "#!git pull"
      ],
      "metadata": {
        "id": "FPyCvGsWhWfK"
      },
      "id": "FPyCvGsWhWfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63edff5a-724b-474d-9db8-37f0ae936c76",
      "metadata": {
        "id": "63edff5a-724b-474d-9db8-37f0ae936c76",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install -q -U langchain_openai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zx1cbqJLLsic",
      "metadata": {
        "id": "Zx1cbqJLLsic"
      },
      "outputs": [],
      "source": [
        "%pip install -U duckduckgo-search wikipedia wikipedia-api duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ksgy3_SDwIQo",
      "metadata": {
        "id": "Ksgy3_SDwIQo"
      },
      "outputs": [],
      "source": [
        "%pip install opencv-python yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lFQQHNkWFq0B",
      "metadata": {
        "id": "lFQQHNkWFq0B"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_huggingface langchain_community datasets gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r9AwfEFgFR6l",
      "metadata": {
        "id": "r9AwfEFgFR6l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import requests\n",
        "import inspect\n",
        "import pandas as pd\n",
        "from typing import TypedDict, Annotated\n",
        "from huggingface_hub import InferenceClient, login, list_models\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
        "#from langchain.schema import AIMessage, HumanMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.docstore.document import Document\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import datasets\n",
        "import re\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.tools import Tool\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
        "import wikipediaapi, wikipedia\n",
        "from duckduckgo_search import DDGS\n",
        "import yt_dlp\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import requests\n",
        "import inspect\n",
        "import pandas as pd\n",
        "from typing import TypedDict, Annotated\n",
        "from huggingface_hub import InferenceClient, login, list_models\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
        "#from langchain.schema import AIMessage, HumanMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.docstore.document import Document\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import datasets\n",
        "import re\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.tools import Tool\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
        "import wikipediaapi, wikipedia\n",
        "from duckduckgo_search import DDGS\n",
        "import yt_dlp\n",
        "import cv2\n",
        "import glob\n",
        "import subprocess\n",
        "from HF_agents.experiments.tools import * # Assuming this file is in a location accessible from the current directory or you've set up the correct import path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K84TxdeBm5s9",
      "metadata": {
        "id": "K84TxdeBm5s9"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AkuieDdHHBf",
      "metadata": {
        "id": "7AkuieDdHHBf"
      },
      "outputs": [],
      "source": [
        "# API endpoint for retrieving the list of questions\n",
        "url = \"https://agents-course-unit4-scoring.hf.space/questions\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    questions = response.json()\n",
        "    print(questions)\n",
        "else:\n",
        "    print(f\"Failed to retrieve questions. Status code: {response.status_code}\")\n",
        "    print(response.text)\n",
        "\n",
        "\n",
        "questions = [item['question'] for item in response.json()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qeDu9pOSnL-h",
      "metadata": {
        "id": "qeDu9pOSnL-h"
      },
      "outputs": [],
      "source": [
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"fetch_wikipedia\", fetch_wikipedia_content)\n",
        "graph_builder.add_node(\"answer_question\", answer_from_article)\n",
        "\n",
        "graph_builder.add_edge(START, \"fetch_wikipedia\")\n",
        "graph_builder.add_edge(\"fetch_wikipedia\", \"answer_question\")\n",
        "graph_builder.add_edge(\"answer_question\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2yOgS3F5UMJv",
      "metadata": {
        "id": "2yOgS3F5UMJv"
      },
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"fetch_wikipedia\", fetch_wikipedia_content)\n",
        "graph_builder.add_node(\"answer_question\", answer_from_article)\n",
        "graph_builder.add_node(\"answer_directly\", answer_directly)  # New node for direct answering\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    START,\n",
        "    # Single condition function that returns boolean\n",
        "    lambda state: should_fetch_wikipedia(state[\"question\"]),\n",
        "    # Map boolean results to node names\n",
        "    {\n",
        "        True: \"fetch_wikipedia\",\n",
        "        False: \"answer_directly\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"fetch_wikipedia\", \"answer_question\")\n",
        "graph_builder.add_edge(\"answer_question\", END)\n",
        "graph_builder.add_edge(\"answer_directly\", END)  # Edge for direct answering\n",
        "\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cHzY_jRZOh_",
      "metadata": {
        "id": "4cHzY_jRZOh_"
      },
      "outputs": [],
      "source": [
        "questions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WVzv2sq-Y9uP",
      "metadata": {
        "id": "WVzv2sq-Y9uP"
      },
      "outputs": [],
      "source": [
        "# Define the initial state with the user's question\n",
        "initial_state = {\n",
        "    \"question\": \"what professional NFL teams did Joe Montana play for?\",  # Replace with your desired question\n",
        "    \"article_content\": \"\",\n",
        "    \"answer\": \"\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
        "}\n",
        "\n",
        "# Invoke the graph with the initial state\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "# Access the answer from the result\n",
        "answer = result[\"answer\"]\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wcHc7WFSnSUe",
      "metadata": {
        "id": "wcHc7WFSnSUe"
      },
      "outputs": [],
      "source": [
        "def run_wikipedia_tool(question):\n",
        "    initial_state = {\n",
        "        \"question\": question,\n",
        "        \"article_content\": \"\",\n",
        "        \"answer\": \"\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
        "    }\n",
        "    result = graph.invoke(initial_state)\n",
        "    return result[\"answer\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GYqRD-LQo_Uo",
      "metadata": {
        "id": "GYqRD-LQo_Uo"
      },
      "outputs": [],
      "source": [
        "answer = run_wikipedia_tool(questions[0])\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GO_kmk05gQ_8",
      "metadata": {
        "id": "GO_kmk05gQ_8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert_webm_to_mp4(input_path, output_path):\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", input_path,\n",
        "        \"-c:v\", \"libx264\",\n",
        "        \"-preset\", \"slow\",\n",
        "        \"-crf\", \"22\",\n",
        "        \"-c:a\", \"aac\",\n",
        "        \"-b:a\", \"128k\",\n",
        "        output_path\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "\n",
        "def download_youtube_video(url, output_dir='/content/video/', output_filename='downloaded_video.mp4'):\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Delete all files in the output directory\n",
        "    files = glob.glob(os.path.join(output_dir, '*'))\n",
        "    for f in files:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {f}: {str(e)}\")\n",
        "\n",
        "    # Set output path for yt-dlp\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': output_path,\n",
        "        'quiet': True,\n",
        "        'merge_output_format': 'mp4',  # Ensures merged output is mp4\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegVideoConvertor',\n",
        "            'preferedformat': 'mp4',  # Recode if needed\n",
        "        }]\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    return output_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rVVZkZ3UIv5U",
      "metadata": {
        "id": "rVVZkZ3UIv5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_frames(video_path, output_dir, frame_interval_seconds=60):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video at specified intervals and saves them to a directory.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        output_dir (str): Directory to save the frames.\n",
        "        frame_interval_seconds (int, optional): Interval between frames in seconds. Defaults to 60.\n",
        "    \"\"\"\n",
        "    # Clear the output directory\n",
        "    for filename in os.listdir(output_dir):\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "    # Open the video file\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    if not vidcap.isOpened():\n",
        "        raise IOError(\"Error opening video file\")\n",
        "\n",
        "    # Get video FPS\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Calculate frame interval in frame count\n",
        "    frame_interval = int(fps * frame_interval_seconds)\n",
        "\n",
        "    # Extract frames\n",
        "    frame_count = 0\n",
        "    success, image = vidcap.read()\n",
        "    while success:\n",
        "        if frame_count % frame_interval == 0:\n",
        "            frame_path = os.path.join(output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            #print(f\"Saved frame {frame_count} to {frame_path}\")\n",
        "        success, image = vidcap.read()\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    vidcap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gIuxFhtOFDuV",
      "metadata": {
        "id": "gIuxFhtOFDuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage\n",
        "youtube_url = \"https://www.youtube.com/watch?v=YTR21os8gTA\"\n",
        "video_file = download_youtube_video(youtube_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lfXP2TVCI-W0",
      "metadata": {
        "id": "lfXP2TVCI-W0"
      },
      "outputs": [],
      "source": [
        "video_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UCUHEUdpSVPs",
      "metadata": {
        "id": "UCUHEUdpSVPs"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r6pSNY0pxmr-",
      "metadata": {
        "id": "r6pSNY0pxmr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "extract_frames(video_path=video_file,\n",
        "               output_dir='/content/frames/',\n",
        "               frame_interval_seconds=10)  # Save one frame every 60 frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K9D7SjKdDlbr",
      "metadata": {
        "id": "K9D7SjKdDlbr"
      },
      "outputs": [],
      "source": [
        "video_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHYSWR_E4IXj",
      "metadata": {
        "id": "AHYSWR_E4IXj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering # Changed imports\n",
        "\n",
        "name_vqa = \"Salesforce/blip-vqa-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AAjzmQe9gIY",
      "metadata": {
        "id": "7AAjzmQe9gIY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize processor and model\n",
        "processor_vqa = BlipProcessor.from_pretrained(name_vqa)\n",
        "model_vqa = BlipForQuestionAnswering.from_pretrained(\n",
        "    name_vqa,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SIF4Ky3J4NcW",
      "metadata": {
        "id": "SIF4Ky3J4NcW"
      },
      "outputs": [],
      "source": [
        "def count_bird_species(image_path):\n",
        "    # Load and process image\n",
        "    raw_image = Image.open(image_path) # raw_image is now loaded within the function using image_path\n",
        "    # Get image size\n",
        "    width, height = raw_image.size\n",
        "\n",
        "    # The prompt should not include the image data directly\n",
        "    prompt = \"How many different types of bird species are in this image?\"\n",
        "\n",
        "    inputs = processor_vqa(raw_image, prompt, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "    out = model_vqa.generate(**inputs)\n",
        "    response = processor_vqa.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ld7weTEsBXJC",
      "metadata": {
        "id": "ld7weTEsBXJC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_directory_images(directory_path):\n",
        "    \"\"\"\n",
        "    Process all images in a directory and store results in a list of dictionaries\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Supported image extensions\n",
        "    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.lower().endswith(valid_extensions):\n",
        "            image_path = os.path.join(directory_path, filename)\n",
        "            try:\n",
        "                res = count_bird_species(image_path)\n",
        "                results.append({\n",
        "                    'filename': filename,\n",
        "                    'response': res\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6Y62XOtBddT",
      "metadata": {
        "id": "E6Y62XOtBddT"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "directory_path = '/content/frames'\n",
        "analysis_results = process_directory_images(directory_path)\n",
        "num_species = [x['response'] for x in analysis_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B-M8iXFCBoep",
      "metadata": {
        "id": "B-M8iXFCBoep"
      },
      "outputs": [],
      "source": [
        "max(num_species)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ahP4-iu4QG7d",
      "metadata": {
        "id": "ahP4-iu4QG7d"
      },
      "outputs": [],
      "source": [
        "def get_wiki_search(inquiry: str) -> str:\n",
        "    \"\"\"Performs a Wikipedia search for the given inquiry.\"\"\"\n",
        "    results = get_wikipedia_article_full_text(inquiry)\n",
        "    if results is not None:\n",
        "        return results\n",
        "    else:\n",
        "        return \"No information found.\"\n",
        "\n",
        "  # Initialize the tool\n",
        "wiki_search_tool = Tool(\n",
        "    name=\"get_wiki_search\",\n",
        "    func=get_wiki_search,\n",
        "    description=\"Performs a Wikipedia search for the given inquiry.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZLcR8lv4PqAa",
      "metadata": {
        "id": "ZLcR8lv4PqAa"
      },
      "outputs": [],
      "source": [
        "def get_web_search(inquiry: str) -> str:\n",
        "    \"\"\"Performs a web search for the given inquiry using the DuckDuckGo engine.\"\"\"\n",
        "\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    return search.run(inquiry)\n",
        "\n",
        "# Initialize the tool\n",
        "web_search_tool = Tool(\n",
        "    name=\"get_web_search\",\n",
        "    func=get_web_search,\n",
        "    description=\"Performs a web search for the given inquiry using the DuckDuckGo engine.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wcJ5U7pWLhtr",
      "metadata": {
        "id": "wcJ5U7pWLhtr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
        "tools = [get_web_search, get_wiki_search]\n",
        "chat_with_tools = chat.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cLuqY7-l9B7",
      "metadata": {
        "id": "8cLuqY7-l9B7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmB8aK7VL5a4",
      "metadata": {
        "id": "qmB8aK7VL5a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate the AgentState and Agent graph\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    response = chat_with_tools.invoke(state[\"messages\"])\n",
        "    # Check if the response is a string and not a tool call\n",
        "    if isinstance(response, str):\n",
        "        # If it's a string, it means the agent provided a final answer\n",
        "        return {\"messages\": [AIMessage(content=response)], \"stop\": True}  # Set 'stop' to True to terminate the graph\n",
        "    else:\n",
        "        # If it's not a string, it's a tool call, continue the graph execution\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "## The graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message requires a tool, route to tools\n",
        "    # Otherwise, provide a direct response\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cY1YoHEHRjFO",
      "metadata": {
        "id": "cY1YoHEHRjFO"
      },
      "outputs": [],
      "source": [
        "questions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wzio9JzwRanc",
      "metadata": {
        "id": "wzio9JzwRanc"
      },
      "outputs": [],
      "source": [
        "wiki_search_tool.invoke(\"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jxLISDbvMrS7",
      "metadata": {
        "id": "jxLISDbvMrS7"
      },
      "outputs": [],
      "source": [
        "def test_my_agent(qnum=0):\n",
        "  print(f\"Question {qnum}: \" + questions[qnum])\n",
        "  response = react_graph.invoke({\"messages\": f\"{questions[qnum]}\"})\n",
        "\n",
        "  print(\"ðŸŽ© Agent's Response:\")\n",
        "  print(response['messages'][-1].content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OxUGSb8pNBur",
      "metadata": {
        "id": "OxUGSb8pNBur"
      },
      "outputs": [],
      "source": [
        "test_my_agent(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U0E-rLvnLz0P",
      "metadata": {
        "id": "U0E-rLvnLz0P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LhLdkP5wMd-A",
      "metadata": {
        "id": "LhLdkP5wMd-A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AmM8IugcIc4K",
      "metadata": {
        "id": "AmM8IugcIc4K"
      },
      "outputs": [],
      "source": [
        "questions[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GHpwExfdJu8Y",
      "metadata": {
        "id": "GHpwExfdJu8Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71795ff1-d6a7-448d-8b55-88bbd1ed3dbe",
      "metadata": {
        "id": "71795ff1-d6a7-448d-8b55-88bbd1ed3dbe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from typing import List\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "vision_llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "def extract_text(img_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Args:\n",
        "        img_path: A local image file path (strings).\n",
        "\n",
        "    Returns:\n",
        "        A single string containing the concatenated text extracted from each image.\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "    try:\n",
        "\n",
        "        # Read image and encode as base64\n",
        "        with open(img_path, \"rb\") as image_file:\n",
        "            image_bytes = image_file.read()\n",
        "\n",
        "        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "        # Prepare the prompt including the base64 image data\n",
        "        message = [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": (\n",
        "                            \"Extract all the text from this image. \"\n",
        "                            \"Return only the extracted text, no explanations.\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
        "                        },\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Call the vision-capable model\n",
        "        response = vision_llm.invoke(message)\n",
        "\n",
        "        # Append extracted text\n",
        "        all_text += response.content + \"\\n\\n\"\n",
        "\n",
        "        return all_text.strip()\n",
        "    except Exception as e:\n",
        "        # You can choose whether to raise or just return an empty string / error message\n",
        "        error_msg = f\"Error extracting text: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return \"\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [\n",
        "    divide,\n",
        "    extract_text\n",
        "]\n",
        "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2cec014-3023-405c-be79-de8fc7adb346",
      "metadata": {
        "id": "a2cec014-3023-405c-be79-de8fc7adb346"
      },
      "source": [
        "Let's create our LLM and prompt it with the overall desired agent behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb674bc-49b2-485a-b0c3-4d7b05a0bfac",
      "metadata": {
        "id": "deb674bc-49b2-485a-b0c3-4d7b05a0bfac",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, List, Any, Optional\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "class AgentState(TypedDict):\n",
        "    # The input document\n",
        "    input_file:  Optional[str]  # Contains file path, type (PNG)\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d061813f-ebc0-432c-91ec-3b42b15c30b6",
      "metadata": {
        "id": "d061813f-ebc0-432c-91ec-3b42b15c30b6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "# AgentState\n",
        "def assistant(state: AgentState):\n",
        "    # System message\n",
        "    textual_description_of_tool=\"\"\"\n",
        "extract_text(img_path: str) -> str:\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Args:\n",
        "        img_path: A local image file path (strings).\n",
        "\n",
        "    Returns:\n",
        "        A single string containing the concatenated text extracted from each image.\n",
        "divide(a: int, b: int) -> float:\n",
        "    Divide a and b\n",
        "\"\"\"\n",
        "    image=state[\"input_file\"]\n",
        "    sys_msg = SystemMessage(content=f\"You are an helpful agent that can analyse some images and run some computatio without provided tools :\\n{textual_description_of_tool} \\n You have access to some otpional images. Currently the loaded images is : {image}\")\n",
        "\n",
        "\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])],\"input_file\":state[\"input_file\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb43343-9a6f-42cb-86e6-4380f928633c",
      "metadata": {
        "id": "4eb43343-9a6f-42cb-86e6-4380f928633c"
      },
      "source": [
        "We define a `Tools` node with our list of tools.\n",
        "\n",
        "The `Assistant` node is just our model with bound tools.\n",
        "\n",
        "We create a graph with `Assistant` and `Tools` nodes.\n",
        "\n",
        "We add `tools_condition` edge, which routes to `End` or to `Tools` based on  whether the `Assistant` calls a tool.\n",
        "\n",
        "Now, we add one new step:\n",
        "\n",
        "We connect the `Tools` node *back* to the `Assistant`, forming a loop.\n",
        "\n",
        "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
        "* If it is a tool call, the flow is directed to the `tools` node.\n",
        "* The `tools` node connects back to `assistant`.\n",
        "* This loop continues as long as the model decides to call tools.\n",
        "* If the model response is not a tool call, the flow is directed to END, terminating the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef13cd4-05a6-4084-a620-2e7b91d9a72f",
      "metadata": {
        "id": "aef13cd4-05a6-4084-a620-2e7b91d9a72f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75602459-d8ca-47b4-9518-3f38343ebfe4",
      "metadata": {
        "id": "75602459-d8ca-47b4-9518-3f38343ebfe4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
        "\n",
        "messages = react_graph.invoke({\"messages\": messages,\"input_file\":None})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b517142d-c40c-48bf-a5b8-c8409427aa79",
      "metadata": {
        "id": "b517142d-c40c-48bf-a5b8-c8409427aa79",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08386393-c270-43a5-bde2-2b4075238971",
      "metadata": {
        "id": "08386393-c270-43a5-bde2-2b4075238971"
      },
      "source": [
        "## Training program\n",
        "MR Wayne left a note with his training program for the week. I came up with a recipe for dinner leaft in a note.\n",
        "\n",
        "you can find the document [HERE](https://huggingface.co/datasets/agents-course/course-images/blob/main/en/unit2/LangGraph/Batman_training_and_meals.png), so download it and upload it in the local folder.\n",
        "\n",
        "![Training](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Batman_training_and_meals.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCOTPGjV94Dl",
      "metadata": {
        "id": "nCOTPGjV94Dl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e97e84-3b05-4aaf-a38f-1de9b73cd37f",
      "metadata": {
        "id": "f6e97e84-3b05-4aaf-a38f-1de9b73cd37f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"According the note provided by MR wayne in the provided images. What's the list of items I should buy for the dinner menu ?\")]\n",
        "\n",
        "messages = react_graph.invoke({\"messages\": messages,\"input_file\":\"Batman_training_and_meals.png\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17686d52-c7ba-407b-a13f-f6c37668e5b0",
      "metadata": {
        "id": "17686d52-c7ba-407b-a13f-f6c37668e5b0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b96c8456-4093-4cd6-bc5a-f611967ab709",
      "metadata": {
        "id": "b96c8456-4093-4cd6-bc5a-f611967ab709"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}